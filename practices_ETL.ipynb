{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "#                  Comprehensive Guide: Data Science in ETL\n",
    "# ==============================================================================\n",
    "# This notebook provides detailed examples of Data Science practices in\n",
    "# each phase of the ETL (Extract, Transform, Load) process.\n",
    "#\n",
    "# Each section includes:\n",
    "# - Practice Description.\n",
    "# - Practical example with a business scenario.\n",
    "# - Executable Python code (with simulated example data where necessary).\n",
    "# - Recommended tools.\n",
    "# - Key objective of the practice.\n",
    "#\n",
    "# Make sure you have the necessary libraries installed!\n",
    "# You can install them with:\n",
    "# pip install pandas numpy requests sqlalchemy\n",
    "# (Note: `psycopg2-binary`, `snowflake-sqlalchemy`, and `boto3` are for real connections\n",
    "# and would require installation. The script uses mocks to run without them).\n",
    "# ==============================================================================\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime, date, timedelta\n",
    "\n",
    "# To simulate psycopg2 and sqlalchemy without a real DB\n",
    "from sqlalchemy import create_engine, text\n",
    "from unittest.mock import MagicMock\n",
    "import os # For deleting temporary files\n",
    "\n",
    "# --- Configuration for Simulations (Mocks) ---\n",
    "# These mocks allow the code to run without real databases or\n",
    "# configured cloud services. If you wish to use real connections, ensure\n",
    "# you have the corresponding libraries installed and comment out/remove\n",
    "# the relevant mocking sections.\n",
    "\n",
    "class MockCursor:\n",
    "    def execute(self, query, params=None):\n",
    "        # print(f\"Mock DB: Executing query: {query}\") # Uncomment to see mock logs\n",
    "        if \"SELECT\" in query.upper():\n",
    "            return self._mock_data()\n",
    "        return None\n",
    "\n",
    "    def fetchall(self):\n",
    "        # Simulated data for a SELECT in Practice 1\n",
    "        return [\n",
    "            (1, 101, datetime(2023, 1, 10), 50.00, 'John Doe', 'john.doe@example.com', 'Laptop', 'Electronics', 1000.00),\n",
    "            (2, 102, datetime(2023, 1, 15), 120.50, 'Jane Smith', 'jane.smith@example.com', 'Mouse', 'Electronics', 25.00),\n",
    "            (3, 101, datetime(2023, 2, 1), 75.00, 'John Doe', 'john.doe@example.com', 'Keyboard', 'Peripherals', 70.00),\n",
    "            (4, 103, datetime(2023, 2, 5), 30.00, 'Alice Wonderland', 'alice@example.com', 'Webcam', 'Accessories', 50.00)\n",
    "        ]\n",
    "\n",
    "    def close(self):\n",
    "        # print(\"Mock DB: Cursor closed.\") # Uncomment to see mock logs\n",
    "        pass\n",
    "\n",
    "    def _mock_data(self):\n",
    "        pass\n",
    "\n",
    "class MockConnection:\n",
    "    def cursor(self):\n",
    "        return MockCursor()\n",
    "    def commit(self):\n",
    "        # print(\"Mock DB: Committing transaction.\") # Uncomment to see mock logs\n",
    "        pass\n",
    "    def close(self):\n",
    "        # print(\"Mock DB: Connection closed.\") # Uncomment to see mock logs\n",
    "        pass\n",
    "\n",
    "def mock_connect(*args, **kwargs):\n",
    "    # print(\"Mock DB: Connecting to database.\") # Uncomment to see mock logs\n",
    "    return MockConnection()\n",
    "\n",
    "# Override `psycopg2.connect` with our mock if psycopg2 is not available\n",
    "try:\n",
    "    import psycopg2\n",
    "    # If psycopg2 is installed, a real connection will be attempted.\n",
    "    # To force the mock even with psycopg2 installed, uncomment the line:\n",
    "    # psycopg2.connect = mock_connect\n",
    "    print(\"`psycopg2` installed. Real connection will be attempted; if it fails, simulated data will be used.\")\n",
    "except ImportError:\n",
    "    print(\"`psycopg2` not installed. Using mock for database connections.\")\n",
    "    import sys\n",
    "    sys.modules['psycopg2'] = MagicMock()\n",
    "    sys.modules['psycopg2.extras'] = MagicMock()\n",
    "    sys.modules['psycopg2'].connect = mock_connect\n",
    "\n",
    "# Mock for SQLAlchemy `create_engine` and `Connection`\n",
    "class MockSAConnection:\n",
    "    def execute(self, statement, parameters=None):\n",
    "        # print(f\"Mock SQLAlchemy: Executing statement: {statement}\") # Uncomment to see logs\n",
    "        if isinstance(statement, text) and \"SELECT\" in statement.text.upper():\n",
    "            return MagicMock(fetchall=lambda: [(\"2023-11-20\",)]) # Simulates a result for SELECT in DELETE\n",
    "        return MagicMock()\n",
    "\n",
    "    def commit(self):\n",
    "        # print(\"Mock SQLAlchemy: Committing transaction.\") # Uncomment to see logs\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        # print(\"Mock SQLAlchemy: Connection closed.\") # Uncomment to see logs\n",
    "        pass\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.close()\n",
    "\n",
    "class MockSAEngine:\n",
    "    def connect(self):\n",
    "        return MockSAConnection()\n",
    "\n",
    "    def dispose(self):\n",
    "        # print(\"Mock SQLAlchemy: Engine disposed.\") # Uncomment to see logs\n",
    "        pass\n",
    "\n",
    "    # `pd.to_sql` expects the `con` object to have an `execute` or `dialect` method\n",
    "    # We simplify this for our mock\n",
    "    def to_sql(self, df, name, con, if_exists, index):\n",
    "        # print(f\"Mock SQLAlchemy: pd.to_sql called. Table: {name}, if_exists: {if_exists}\") # Uncomment to see logs\n",
    "        pass # Simulates writing\n",
    "\n",
    "# Replace the actual SQLAlchemy create_engine function with our mock\n",
    "_original_create_engine = create_engine\n",
    "create_engine = lambda *args, **kwargs: MockSAEngine()\n",
    "\n",
    "# Replace `pd.DataFrame.to_sql` with a mock if you're using the mock engine\n",
    "_original_to_sql_method = pd.DataFrame.to_sql\n",
    "def mocked_to_sql(df_self, name, con, if_exists='fail', index=True, chunksize=None, dtype=None, method=None):\n",
    "    if isinstance(con, MockSAEngine) or (hasattr(con, 'connect') and isinstance(con.connect(), MockSAConnection)):\n",
    "        con.to_sql(df_self, name, con, if_exists, index) # Calls the mock engine's to_sql method\n",
    "    else:\n",
    "        _original_to_sql_method(df_self, name, con, if_exists, index, chunksize, dtype, method)\n",
    "pd.DataFrame.to_sql = mocked_to_sql\n",
    "\n",
    "\n",
    "# Mock for boto3 (AWS S3) if you don't have AWS credentials configured\n",
    "try:\n",
    "    import boto3\n",
    "    # If boto3 is installed, a real connection will be attempted.\n",
    "    # To force the mock even with boto3 installed, uncomment the line:\n",
    "    # boto3.client = MagicMock(return_value=MagicMock(upload_file=lambda *args, **kwargs: print(f\"Mock S3: Uploading file {args[0]} to s3://{args[1]}/{args[2]}\")))\n",
    "    print(\"`boto3` installed. Real connection will be attempted; if it fails, it will be reported.\")\n",
    "except ImportError:\n",
    "    print(\"`boto3` not installed. Using mock for S3 operations.\")\n",
    "    import sys\n",
    "    sys.modules['boto3'] = MagicMock()\n",
    "    sys.modules['boto3'].client = MagicMock(return_value=MagicMock(upload_file=lambda *args, **kwargs: print(f\"Mock S3: Uploading file {args[0]} to s3://{args[1]}/{args[2]}\")))\n",
    "\n",
    "print(\"\\n--- Mocks configured for execution without external dependencies if not installed! ---\")\n",
    "print(\"If you wish to use real connections, ensure you have the libraries installed and adjust mocking and credential sections.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Guide: Data Science in ETL\n",
    "\n",
    "The ETL (Extract, Transform, Load) stage is fundamental in any Data Science project. It ensures that data is clean, consistent, and ready for analysis and modeling. This notebook will guide you through detailed examples of Data Science practices in each ETL phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extract Phase\n",
    "\n",
    "**Objective:** Collect data from various sources and formats, ensuring its integrity.\n",
    "\n",
    "### Practice 1: Extracting Data from a Relational Database\n",
    "\n",
    "**Description:** Connect to an SQL database to extract specific tables or results from complex queries.\n",
    "\n",
    "**Practical Example:**\n",
    "Imagine you are a Data Scientist at an e-commerce company and need to analyze customer purchasing behavior. Transaction data is stored in a PostgreSQL database.\n",
    "\n",
    "*   **Data Sources:** PostgreSQL database (`orders` table, `customers` table, `products` table).\n",
    "*   **Data to Extract:** `order_id`, `customer_id`, `order_date`, `total_amount` from `orders`, along with customer and product details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- PHASE 1: EXTRACT ---\\n\")\n",
    "\n",
    "### Practice 1: Extracting Data from a Relational Database ###\n",
    "\n",
    "print(\"### Practice 1: Extracting Data from a Relational Database ###\")\n",
    "\n",
    "# Connection configuration (using mock if no real DB exists)\n",
    "# If you have a real DB (e.g., PostgreSQL), replace with your credentials:\n",
    "db_config = {\n",
    "    'host': 'localhost',\n",
    "    'database': 'testdb',\n",
    "    'user': 'user',\n",
    "    'password': 'password'\n",
    "}\n",
    "\n",
    "# SQL Extraction Query\n",
    "sql_query = \"\"\"\n",
    "SELECT\n",
    "    o.order_id,\n",
    "    o.customer_id,\n",
    "    o.order_date,\n",
    "    o.total_amount,\n",
    "    c.name AS customer_name,\n",
    "    c.email AS customer_email,\n",
    "    p.product_name,\n",
    "    p.category,\n",
    "    p.price\n",
    "FROM orders o\n",
    "JOIN customers c ON o.customer_id = c.customer_id\n",
    "JOIN order_items oi ON o.order_id = oi.order_id\n",
    "JOIN products p ON oi.product_id = p.product_id\n",
    "WHERE o.order_date >= '2023-01-01';\n",
    "\"\"\"\n",
    "\n",
    "df_raw_orders = pd.DataFrame() # Initialize\n",
    "\n",
    "try:\n",
    "    # Attempt real connection with psycopg2 if available, otherwise use mock\n",
    "    import psycopg2 # Import here to handle ImportError from mock\n",
    "    conn = psycopg2.connect(**db_config)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(sql_query)\n",
    "    # Get column names from cursor description\n",
    "    column_names = [desc[0] for desc in cur.description]\n",
    "    raw_data = cur.fetchall()\n",
    "    df_raw_orders = pd.DataFrame(raw_data, columns=column_names)\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    print(\"Database extraction successful (simulated or real).\")\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to DB or executing query: {e}. Using simulated data.\")\n",
    "    # Simulate data if connection fails or mock is active\n",
    "    df_raw_orders = pd.DataFrame({\n",
    "        'order_id': [1, 2, 3, 4],\n",
    "        'customer_id': [101, 102, 101, 103],\n",
    "        'order_date': [datetime(2023, 1, 10), datetime(2023, 1, 15), datetime(2023, 2, 1), datetime(2023, 2, 5)],\n",
    "        'total_amount': [50.00, 120.50, 75.00, 30.00],\n",
    "        'customer_name': ['John Doe', 'Jane Smith', 'John Doe', 'Alice Wonderland'],\n",
    "        'customer_email': ['john.doe@example.com', 'jane.smith@example.com', 'john.doe@example.com', 'alice@example.com'],\n",
    "        'product_name': ['Laptop', 'Mouse', 'Keyboard', 'Webcam'],\n",
    "        'category': ['Electronics', 'Electronics', 'Peripherals', 'Accessories'],\n",
    "        'price': [1000.00, 25.00, 70.00, 50.00]\n",
    "    })\n",
    "\n",
    "print(\"\\nFirst rows of data extracted from the database:\")\n",
    "print(df_raw_orders.head())\n",
    "\n",
    "# Tools to Use: Python (psycopg2, sqlalchemy), Apache Airflow, DBeaver.\n",
    "# Objective: Obtain a flat and combined dataset with order, customer, and product information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice 2: Extracting Data from a Web API\n",
    "\n",
    "**Description:** Connect to an API (Application Programming Interface) to obtain data in JSON or XML format.\n",
    "\n",
    "**Practical Example:**\n",
    "You are a Data Scientist at a marketing company and need to get Google Trends search trend data for a specific set of keywords.\n",
    "\n",
    "*   **Data Sources:** Google Trends API (or a similar API).\n",
    "*   **Data to Extract:** Search volumes by keyword, region, and time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n### Practice 2: Extracting Data from a Web API ###\")\n",
    "\n",
    "keywords = ['data science', 'machine learning', 'artificial intelligence']\n",
    "start_date_api = '2023-01-01'\n",
    "end_date_api = '2023-01-07'\n",
    "api_key = \"YOUR_API_KEY\" # Replace if the real API requires authentication\n",
    "\n",
    "all_trends_data = []\n",
    "\n",
    "print(f\"\\nExtracting API data for keywords: {', '.join(keywords)}\")\n",
    "\n",
    "# API response simulation\n",
    "mock_api_responses = {\n",
    "    'data science': {'trend_score': [80, 85, 82, 88, 90, 87, 91], 'dates': [f'2023-01-0{i+1}' for i in range(7)]},\n",
    "    'machine learning': {'trend_score': [70, 72, 75, 71, 78, 76, 79], 'dates': [f'2023-01-0{i+1}' for i in range(7)]},\n",
    "    'artificial intelligence': {'trend_score': [95, 93, 98, 96, 99, 94, 97], 'dates': [f'2023-01-0{i+1}' for i in range(7)]}\n",
    "}\n",
    "\n",
    "for keyword in keywords:\n",
    "    url = f\"https://api.example.com/trends?keyword={keyword}&start_date={start_date_api}&end_date={end_date_api}&api_key={api_key}\"\n",
    "    try:\n",
    "        # In a real case, you would use requests.get(url)\n",
    "        # response = requests.get(url)\n",
    "        # if response.status_code == 200:\n",
    "        #     trend_data = response.json()\n",
    "        if keyword in mock_api_responses:\n",
    "            trend_data = mock_api_responses[keyword]\n",
    "            all_trends_data.append({'keyword': keyword, 'data': trend_data})\n",
    "            # print(f\"  - Data for '{keyword}' successfully obtained (simulated).\") # Uncomment to see logs\n",
    "        else:\n",
    "            print(f\"  - Simulated error: No data found for '{keyword}'.\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"  - Connection error for {keyword}: {e}\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"  - JSON decoding error for {keyword}.\")\n",
    "\n",
    "# Convert data to a DataFrame for better visualization\n",
    "df_trends_list = []\n",
    "for entry in all_trends_data:\n",
    "    keyword = entry['keyword']\n",
    "    data = entry['data']\n",
    "    for i in range(len(data['dates'])):\n",
    "        df_trends_list.append({\n",
    "            'keyword': keyword,\n",
    "            'date': pd.to_datetime(data['dates'][i]),\n",
    "            'trend_score': data['trend_score'][i]\n",
    "        })\n",
    "df_trends = pd.DataFrame(df_trends_list)\n",
    "\n",
    "print(\"\\nFirst rows of trend data (simulated API):\")\n",
    "print(df_trends.head())\n",
    "\n",
    "# Tools to Use: Python (requests, json), pytrends, Apache Airflow.\n",
    "# Objective: Obtain search trend data to identify patterns of interest.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Transform Phase\n",
    "\n",
    "**Objective:** Clean, enrich, and structure data to make it suitable for analysis.\n",
    "\n",
    "### Practice 3: Data Cleaning and Standardization\n",
    "\n",
    "**Description:** Identify and correct errors, inconsistencies, and missing values, and standardize formats.\n",
    "\n",
    "**Practical Example:**\n",
    "After extracting customer data from the database (Example 1), you notice that customer names have typos, email addresses might be in mixed case, and registration dates have varied formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- PHASE 2: TRANSFORM ---\\n\")\n",
    "\n",
    "### Practice 3: Data Cleaning and Standardization ###\n",
    "\n",
    "print(\"### Practice 3: Data Cleaning and Standardization ###\")\n",
    "\n",
    "# Simulated input data (with inconsistencies)\n",
    "data_customers_raw = {\n",
    "    'customer_id': [1, 2, 3, 4, 5, 6],\n",
    "    'customer_name': [\"John Doe\", \"  jane smith  \", \"Alice B.\", \"BOB BROWN\", \"Charlie D.\", \"Eve\"],\n",
    "    'customer_email': [\"john.doe@example.com\", \"JANE.SMITH@EXAMPLE.COM\", \"aliceb@example.com\", \"bob.brown@example.com\", None, \"eve@example.com \"],\n",
    "    'registration_date': [\"2023-01-15 10:30:00\", \"Jan 16, 2023\", \"17/01/2023\", \"2023-01-18\", None, \"2023-01-19\"]\n",
    "}\n",
    "df_customers_raw = pd.DataFrame(data_customers_raw)\n",
    "\n",
    "print(\"Raw Customer DataFrame (with inconsistencies):\")\n",
    "print(df_customers_raw)\n",
    "\n",
    "# Perform transformations\n",
    "df_cleaned_customers = df_customers_raw.copy()\n",
    "\n",
    "# 1. Remove extra whitespace from `customer_name` and `customer_email`\n",
    "df_cleaned_customers['customer_name'] = df_cleaned_customers['customer_name'].str.strip()\n",
    "df_cleaned_customers['customer_email'] = df_cleaned_customers['customer_email'].str.strip()\n",
    "\n",
    "# 2. Convert email addresses to lowercase\n",
    "df_cleaned_customers['customer_email'] = df_cleaned_customers['customer_email'].str.lower()\n",
    "\n",
    "# 3. Standardize date format and handle nulls\n",
    "df_cleaned_customers['registration_date'] = pd.to_datetime(df_cleaned_customers['registration_date'], errors='coerce')\n",
    "df_cleaned_customers['registration_date'].fillna(datetime.now(), inplace=True) # Impute with current date\n",
    "df_cleaned_customers['registration_date'] = df_cleaned_customers['registration_date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# 4. Impute missing values in `customer_email` (e.g., if None after strip)\n",
    "df_cleaned_customers['customer_email'].fillna('unknown@example.com', inplace=True)\n",
    "\n",
    "# 5. (Optional) Standardize name capitalization (e.g., Title Case)\n",
    "df_cleaned_customers['customer_name'] = df_cleaned_customers['customer_name'].str.title()\n",
    "\n",
    "print(\"\\nCleaned and standardized Customer DataFrame:\")\n",
    "print(df_cleaned_customers)\n",
    "\n",
    "# Tools to Use: Python (pandas), Talend, OpenRefine.\n",
    "# Objective: Ensure customer data is consistent and high-quality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice 4: Data Enrichment and Feature Creation\n",
    "\n",
    "**Description:** Combine data from different sources and generate new features that can be useful for modeling.\n",
    "\n",
    "**Practical Example:**\n",
    "You have transaction data (Example 1) and want to calculate customer metrics like \"customer lifetime value\" (CLV) or \"purchase frequency.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n### Practice 4: Data Enrichment and Feature Creation ###\")\n",
    "\n",
    "# Example order data (using df_raw_orders from Example 1 for more realism)\n",
    "df_orders_for_features = df_raw_orders[['customer_id', 'order_date', 'total_amount']].copy()\n",
    "df_orders_for_features['order_date'] = pd.to_datetime(df_orders_for_features['order_date'])\n",
    "\n",
    "# Add some additional data to show more features\n",
    "new_orders_data = pd.DataFrame({\n",
    "    'customer_id': [101, 102, 103],\n",
    "    'order_date': [datetime(2023, 3, 1), datetime(2023, 3, 10), datetime(2023, 3, 15)],\n",
    "    'total_amount': [90.00, 150.00, 60.00]\n",
    "})\n",
    "df_orders_for_features = pd.concat([df_orders_for_features, new_orders_data], ignore_index=True)\n",
    "df_orders_for_features['order_date'] = pd.to_datetime(df_orders_for_features['order_date']) # Ensure datetime type\n",
    "\n",
    "print(\"Order DataFrame for feature engineering:\")\n",
    "print(df_orders_for_features)\n",
    "\n",
    "# Current date for calculating tenure and recency. Fix a date for reproducibility.\n",
    "current_analysis_date = pd.to_datetime('2023-04-01')\n",
    "\n",
    "# 1. Total number of orders and total amount spent per customer\n",
    "df_customer_features = df_orders_for_features.groupby('customer_id')['total_amount'].agg(\n",
    "    num_orders=('total_amount', 'count'),\n",
    "    total_spent=('total_amount', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# 2. Average spent per order\n",
    "df_customer_features['avg_spent_per_order'] = df_customer_features['total_spent'] / df_customer_features['num_orders']\n",
    "\n",
    "# 3. Customer Tenure (days since first order)\n",
    "first_order_date = df_orders_for_features.groupby('customer_id')['order_date'].min().reset_index()\n",
    "first_order_date.rename(columns={'order_date': 'first_order_date'}, inplace=True)\n",
    "df_customer_features = pd.merge(df_customer_features, first_order_date, on='customer_id')\n",
    "df_customer_features['customer_tenure_days'] = (current_analysis_date - df_customer_features['first_order_date']).dt.days\n",
    "\n",
    "# 4. Calculate Recency (days since last order)\n",
    "last_order_date = df_orders_for_features.groupby('customer_id')['order_date'].max().reset_index()\n",
    "last_order_date.rename(columns={'order_date': 'last_order_date'}, inplace=True)\n",
    "df_customer_features = pd.merge(df_customer_features, last_order_date, on='customer_id')\n",
    "df_customer_features['recency_days'] = (current_analysis_date - df_customer_features['last_order_date']).dt.days\n",
    "\n",
    "# 5. Basic RFM Segmentation (Recency, Frequency, Monetary)\n",
    "# For the example, we will use quartiles.\n",
    "# Ensure there are enough unique values for qcut.\n",
    "# If there are few values, qcut may fail. `duplicates='drop'` helps handle this.\n",
    "for col in ['recency_days', 'num_orders', 'total_spent']:\n",
    "    if df_customer_features[col].nunique() < 4:\n",
    "        print(f\"Warning: Not enough unique values in '{col}' for a 4-quartile qcut. Assigning values in a simplified way.\")\n",
    "        df_customer_features[f'{col}_score'] = pd.qcut(df_customer_features[col], q=df_customer_features[col].nunique(), labels=False, duplicates='drop') + 1\n",
    "    else:\n",
    "        labels_q = [1, 2, 3, 4]\n",
    "        if col == 'recency_days': # Lower recency is better, so we invert labels\n",
    "            labels_q = [4, 3, 2, 1]\n",
    "        df_customer_features[f'{col}_score'] = pd.qcut(df_customer_features[col], 4, labels=labels_q, duplicates='drop')\n",
    "\n",
    "df_customer_features['rfm_score'] = df_customer_features['recency_days_score'].astype(str) + \\\n",
    "                                   df_customer_features['num_orders_score'].astype(str) + \\\n",
    "                                   df_customer_features['total_spent_score'].astype(str)\n",
    "\n",
    "\n",
    "print(\"\\nDataFrame with enriched customer features:\")\n",
    "print(df_customer_features)\n",
    "\n",
    "# Tools to Use: Python (pandas, numpy, scikit-learn), Snowflake, Apache Spark.\n",
    "# Objective: Create enriched features for segmentation models, CLV, churn, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice 5: Data Aggregation and Summarization\n",
    "\n",
    "**Description:** Reduce data granularity by summarizing it to a higher level to facilitate analysis or populate dashboards.\n",
    "\n",
    "**Practical Example:**\n",
    "You need to build a dashboard that shows daily and monthly sales by product category. Transaction data is at the order item level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n### Practice 5: Data Aggregation and Summarization ###\")\n",
    "\n",
    "# Simulated input data (order items and products)\n",
    "data_items = {\n",
    "    'order_date': [datetime(2023, 1, 1), datetime(2023, 1, 1), datetime(2023, 1, 2), datetime(2023, 1, 2), datetime(2023, 2, 1), datetime(2023, 2, 1)],\n",
    "    'product_id': [10, 20, 10, 30, 20, 10],\n",
    "    'quantity': [2, 1, 1, 3, 2, 1],\n",
    "    'price': [10.00, 25.00, 10.00, 5.00, 25.00, 10.00]\n",
    "}\n",
    "df_order_items = pd.DataFrame(data_items)\n",
    "\n",
    "data_products_agg = {\n",
    "    'product_id': [10, 20, 30],\n",
    "    'product_name': ['Laptop', 'Mouse', 'Keyboard'],\n",
    "    'category': ['Electronics', 'Electronics', 'Peripherals']\n",
    "}\n",
    "df_products_agg = pd.DataFrame(data_products_agg)\n",
    "\n",
    "print(\"Order Items DataFrame:\")\n",
    "print(df_order_items)\n",
    "print(\"\\nProducts DataFrame:\")\n",
    "print(df_products_agg)\n",
    "\n",
    "# 1. Join `df_order_items` with `df_products_agg` to get the category\n",
    "df_merged_sales_agg = pd.merge(df_order_items, df_products_agg, on='product_id')\n",
    "\n",
    "# 2. Calculate revenue per item\n",
    "df_merged_sales_agg['revenue'] = df_merged_sales_agg['quantity'] * df_merged_sales_agg['price']\n",
    "\n",
    "print(\"\\nSales DataFrame with categories and revenue:\")\n",
    "print(df_merged_sales_agg)\n",
    "\n",
    "# 3. Daily aggregation by category\n",
    "df_daily_revenue = df_merged_sales_agg.groupby([df_merged_sales_agg['order_date'].dt.date, 'category'])['revenue'].sum().reset_index()\n",
    "df_daily_revenue.rename(columns={'order_date': 'date'}, inplace=True)\n",
    "print(\"\\nDaily Revenue by Category:\")\n",
    "print(df_daily_revenue)\n",
    "\n",
    "# 4. Monthly aggregation by category\n",
    "df_merged_sales_agg['month'] = df_merged_sales_agg['order_date'].dt.to_period('M')\n",
    "df_monthly_revenue = df_merged_sales_agg.groupby(['month', 'category'])['revenue'].sum().reset_index()\n",
    "print(\"\\nMonthly Revenue by Category:\")\n",
    "print(df_monthly_revenue)\n",
    "\n",
    "# Tools to Use: Python (pandas), Snowflake, Tableau.\n",
    "# Objective: Provide summarized data for BI dashboards.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Phase\n",
    "\n",
    "**Objective:** Store the transformed data in a suitable destination for final use, whether it's a data warehouse, data lake, or file system.\n",
    "\n",
    "### Practice 6: Incremental Loading to a Data Warehouse\n",
    "\n",
    "**Description:** Insert only new or modified records into the destination, instead of reloading all data each time. This is efficient for large volumes of constantly changing data.\n",
    "\n",
    "**Practical Example:**\n",
    "After transforming daily transaction data (Example 5), you need to load it into a fact table (`fact_sales`) in your Data Warehouse (e.g., Snowflake). You only want to add records for the current day, not duplicate historical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- PHASE 3: LOAD ---\\n\")\n",
    "\n",
    "### Practice 6: Incremental Loading to a Data Warehouse ###\n",
    "\n",
    "print(\"### Practice 6: Incremental Loading to a Data Warehouse ###\")\n",
    "\n",
    "# Transformed daily sales data (using df_daily_revenue from Example 5)\n",
    "df_daily_sales_to_load = df_daily_revenue.copy()\n",
    "df_daily_sales_to_load['sale_date'] = df_daily_sales_to_load['date'] # Rename for consistency\n",
    "del df_daily_sales_to_load['date'] # Remove original column\n",
    "\n",
    "# Add an additional metric column for more realism\n",
    "df_daily_sales_to_load['num_transactions'] = (df_daily_sales_to_load['total_revenue'] / 10).astype(int) # Simulated\n",
    "df_daily_sales_to_load.rename(columns={'total_revenue': 'total_revenue'}, inplace=True) # Keep name\n",
    "\n",
    "print(\"\\nDaily data to load into the Data Warehouse:\")\n",
    "print(df_daily_sales_to_load)\n",
    "\n",
    "# Connection configuration to Snowflake (or any other SQL DB)\n",
    "# Replace with real credentials if a real connection is desired.\n",
    "user = \"YOUR_SNOWFLAKE_USER\"\n",
    "password = \"YOUR_SNOWFLAKE_PASSWORD\"\n",
    "account = \"YOUR_SNOWFLAKE_ACCOUNT\"\n",
    "warehouse = \"YOUR_SNOWFLAKE_WAREHOUSE\"\n",
    "database = \"YOUR_SNOWFLAKE_DATABASE\"\n",
    "schema = \"YOUR_SNOWFLAKE_SCHEMA\"\n",
    "\n",
    "# Create the SQLAlchemy connection URL. Will use the mock if not configured.\n",
    "snowflake_url = f\"snowflake://{user}:{password}@{account}/{database}/{schema}?warehouse={warehouse}\"\n",
    "engine = create_engine(snowflake_url) # This will use our mock if no real connection exists\n",
    "\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        # Date of the data to be loaded (take the first date from the DataFrame)\n",
    "        load_date = df_daily_sales_to_load['sale_date'].iloc[0].strftime('%Y-%m-%d')\n",
    "        print(f\"\\nProcessing incremental load for date: {load_date}\")\n",
    "\n",
    "        # 1. Delete existing data for the current date (for idempotency)\n",
    "        # This is a \"delete-then-insert\" or \"upsert\" pattern if the DB supports it.\n",
    "        delete_sql = text(f\"DELETE FROM fact_sales WHERE sale_date = '{load_date}';\")\n",
    "        connection.execute(delete_sql)\n",
    "        connection.commit() # Commit deletion (in mock this does nothing real)\n",
    "\n",
    "        print(f\"Deleted existing data (if any) for {load_date} in 'fact_sales' (simulated).\")\n",
    "\n",
    "        # 2. Insert the new records\n",
    "        df_daily_sales_to_load.to_sql('fact_sales', con=connection, if_exists='append', index=False)\n",
    "        connection.commit() # Commit insertion (in mock this does nothing real)\n",
    "\n",
    "        print(f\"Loaded new data for {load_date} into 'fact_sales' (simulated).\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during Data Warehouse load: {e}\")\n",
    "    print(\"Ensure DW credentials and configuration if attempting a real connection.\")\n",
    "finally:\n",
    "    if 'engine' in locals() and engine:\n",
    "        engine.dispose()\n",
    "    print(\"Data Warehouse connection closed (simulated or real).\")\n",
    "\n",
    "# Tools to Use: Python (pandas, sqlalchemy), Snowflake, Apache Airflow.\n",
    "# Objective: Keep the Data Warehouse updated efficiently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice 7: Loading to a Data Lake\n",
    "\n",
    "**Description:** Store raw or semi-structured data in its original or near-original format, typically in a distributed file system (like HDFS) or object storage (like S3).\n",
    "\n",
    "**Practical Example:**\n",
    "You want to store raw Google Trends API data (Example 2) in an S3 bucket for future analysis or to be processed by other Big Data tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n### Practice 7: Loading to a Data Lake ###\")\n",
    "\n",
    "# Example Google Trends data (using all_trends_data from Example 2)\n",
    "print(\"\\nTrend data to load into the Data Lake (JSON Format):\")\n",
    "print(json.dumps(all_trends_data, indent=2))\n",
    "\n",
    "# Bucket name and folder prefix (e.g., based on load date)\n",
    "bucket_name = 'your-data-lake-bucket-name' # IMPORTANT! Replace with a real S3 bucket if you use it.\n",
    "current_date_str = datetime.now().strftime('%Y-%m-%d')\n",
    "file_name = f\"google_trends_data_{current_date_str}.json\"\n",
    "local_file_path = f\"/tmp/{file_name}\" # Temporary path for the file\n",
    "s3_key = f\"raw_data/google_trends/{current_date_str}/{file_name}\" # Path in S3\n",
    "\n",
    "# Create the /tmp directory if it doesn't exist (important in some environments like notebooks)\n",
    "os.makedirs('/tmp', exist_ok=True)\n",
    "\n",
    "# Save data to a local temporary file before uploading\n",
    "try:\n",
    "    with open(local_file_path, 'w') as f:\n",
    "        json.dump(all_trends_data, f, indent=4)\n",
    "    print(f\"\\nTemporary file created at: {local_file_path}\")\n",
    "\n",
    "    # Initialize S3 client (will use mock if boto3 is not configured/installed)\n",
    "    s3 = boto3.client('s3',\n",
    "                      aws_access_key_id=\"YOUR_AWS_ACCESS_KEY\",       # Replace with real credentials\n",
    "                      aws_secret_access_key=\"YOUR_AWS_SECRET_KEY\",   # Replace with real credentials\n",
    "                      region_name=\"YOUR_AWS_REGION\")                 # Replace with real region\n",
    "\n",
    "    # Upload the file to S3\n",
    "    s3.upload_file(local_file_path, bucket_name, s3_key)\n",
    "    print(f\"File '{file_name}' successfully uploaded to s3://{bucket_name}/{s3_key} (simulated or real).\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error uploading file to S3: {e}\")\n",
    "    print(\"Ensure AWS credentials are configured (env variables, ~/.aws/credentials) or replace with your keys and that the bucket exists.\")\n",
    "finally:\n",
    "    # Clean up the temporary file\n",
    "    if os.path.exists(local_file_path):\n",
    "        os.remove(local_file_path)\n",
    "        print(f\"Temporary file '{local_file_path}' deleted.\")\n",
    "\n",
    "# Tools to Use: Python (boto3), Amazon S3, Apache Airflow.\n",
    "# Objective: Store raw data in a flexible and scalable format.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Considerations for Data Science in ETL:\n",
    "\n",
    "\n",
    "*   **Data Quality:** A robust ETL is the foundation for accurate ML models. Garbage in results in garbage out.\n",
    "*   **Observability:** Monitoring ETL pipelines to detect failures, latencies, or data quality issues is crucial.\n",
    "*   **Versioning:** Versioning ETL code, data schemas, and the data itself helps with reproducibility.\n",
    "*   **Idempotency:** Ensuring that re-running ETL does not cause unwanted side effects (e.g., data duplication).\n",
    "*   **Scalability:** Designing ETL to handle increasing data volumes.\n",
    "*   **Automation:** Using orchestrators to automate scheduled pipeline execution.\n",
    "*   **Documentation:** Documenting each ETL step, sources, transformations, and destinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- END OF SCRIPT ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
