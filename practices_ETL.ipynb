{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "#                      Guía Completa: Data Science en ETL\n",
    "# ==============================================================================\n",
    "# Este notebook proporciona ejemplos detallados de prácticas de Data Science en\n",
    "# cada fase del proceso ETL (Extract, Transform, Load).\n",
    "#\n",
    "# Cada sección incluye:\n",
    "# - Descripción de la práctica.\n",
    "# - Ejemplo práctico con un escenario de negocio.\n",
    "# - Código Python ejecutable (con datos de ejemplo simulados cuando es necesario).\n",
    "# - Herramientas recomendadas.\n",
    "# - Objetivo clave de la práctica.\n",
    "#\n",
    "# ¡Asegúrate de tener las bibliotecas necesarias instaladas!\n",
    "# Puedes instalarlas con:\n",
    "# pip install pandas numpy requests sqlalchemy\n",
    "# (Nota: `psycopg2-binary`, `snowflake-sqlalchemy` y `boto3` son para conexiones reales\n",
    "# y requerirían instalación. El script usa mocks para ejecutarse sin ellos).\n",
    "# ==============================================================================\n",
    "\n",
    "# Importar bibliotecas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime, date, timedelta\n",
    "\n",
    "# Para simular psycopg2 y sqlalchemy sin una DB real\n",
    "from sqlalchemy import create_engine, text\n",
    "from unittest.mock import MagicMock\n",
    "import os # Para eliminar archivos temporales\n",
    "\n",
    "# --- Configuración para simulaciones (Mocks) ---\n",
    "# Estos mocks permiten que el código se ejecute sin bases de datos reales o\n",
    "# servicios cloud configurados. Si deseas usar conexiones reales, asegúrate\n",
    "# de tener las bibliotecas correspondientes instaladas y de comentar/eliminar\n",
    "# las secciones de mockeo relevantes.\n",
    "\n",
    "class MockCursor:\n",
    "    def execute(self, query, params=None):\n",
    "        # print(f\"Mock DB: Executing query: {query}\") # Descomentar para ver logs del mock\n",
    "        if \"SELECT\" in query.upper():\n",
    "            return self._mock_data()\n",
    "        return None\n",
    "\n",
    "    def fetchall(self):\n",
    "        # Datos simulados para una SELECT en la Práctica 1\n",
    "        return [\n",
    "            (1, 101, datetime(2023, 1, 10), 50.00, 'John Doe', 'john.doe@example.com', 'Laptop', 'Electronics', 1000.00),\n",
    "            (2, 102, datetime(2023, 1, 15), 120.50, 'Jane Smith', 'jane.smith@example.com', 'Mouse', 'Electronics', 25.00),\n",
    "            (3, 101, datetime(2023, 2, 1), 75.00, 'John Doe', 'john.doe@example.com', 'Keyboard', 'Peripherals', 70.00),\n",
    "            (4, 103, datetime(2023, 2, 5), 30.00, 'Alice Wonderland', 'alice@example.com', 'Webcam', 'Accessories', 50.00)\n",
    "        ]\n",
    "\n",
    "    def close(self):\n",
    "        # print(\"Mock DB: Cursor closed.\") # Descomentar para ver logs del mock\n",
    "        pass\n",
    "\n",
    "    def _mock_data(self):\n",
    "        pass\n",
    "\n",
    "class MockConnection:\n",
    "    def cursor(self):\n",
    "        return MockCursor()\n",
    "    def commit(self):\n",
    "        # print(\"Mock DB: Committing transaction.\") # Descomentar para ver logs del mock\n",
    "        pass\n",
    "    def close(self):\n",
    "        # print(\"Mock DB: Connection closed.\") # Descomentar para ver logs del mock\n",
    "        pass\n",
    "\n",
    "def mock_connect(*args, **kwargs):\n",
    "    # print(\"Mock DB: Connecting to database.\") # Descomentar para ver logs del mock\n",
    "    return MockConnection()\n",
    "\n",
    "# Sobrescribir `psycopg2.connect` con nuestro mock si psycopg2 no está disponible\n",
    "try:\n",
    "    import psycopg2\n",
    "    # Si psycopg2 está instalado, se intentará una conexión real.\n",
    "    # Para forzar el mock incluso con psycopg2 instalado, descomenta la línea:\n",
    "    # psycopg2.connect = mock_connect\n",
    "    print(\"`psycopg2` instalado. Se intentará conexión real, si falla se usarán datos simulados.\")\n",
    "except ImportError:\n",
    "    print(\"`psycopg2` no instalado. Usando mock para conexiones a base de datos.\")\n",
    "    import sys\n",
    "    sys.modules['psycopg2'] = MagicMock()\n",
    "    sys.modules['psycopg2.extras'] = MagicMock()\n",
    "    sys.modules['psycopg2'].connect = mock_connect\n",
    "\n",
    "# Mock para SQLAlchemy `create_engine` y `Connection`\n",
    "class MockSAConnection:\n",
    "    def execute(self, statement, parameters=None):\n",
    "        # print(f\"Mock SQLAlchemy: Executing statement: {statement}\") # Descomentar para ver logs\n",
    "        if isinstance(statement, text) and \"SELECT\" in statement.text.upper():\n",
    "            return MagicMock(fetchall=lambda: [(\"2023-11-20\",)]) # Simula un resultado para SELECT en DELETE\n",
    "        return MagicMock()\n",
    "\n",
    "    def commit(self):\n",
    "        # print(\"Mock SQLAlchemy: Committing transaction.\") # Descomentar para ver logs\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        # print(\"Mock SQLAlchemy: Connection closed.\") # Descomentar para ver logs\n",
    "        pass\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.close()\n",
    "\n",
    "class MockSAEngine:\n",
    "    def connect(self):\n",
    "        return MockSAConnection()\n",
    "\n",
    "    def dispose(self):\n",
    "        # print(\"Mock SQLAlchemy: Engine disposed.\") # Descomentar para ver logs\n",
    "        pass\n",
    "\n",
    "    # `pd.to_sql` espera que el objeto `con` tenga un método `execute` o `dialect`\n",
    "    # Simplificamos esto para nuestro mock\n",
    "    def to_sql(self, df, name, con, if_exists, index):\n",
    "        # print(f\"Mock SQLAlchemy: pd.to_sql called. Table: {name}, if_exists: {if_exists}\") # Descomentar para ver logs\n",
    "        pass # Simula la escritura\n",
    "\n",
    "# Reemplazar la función real create_engine de SQLAlchemy con nuestro mock\n",
    "_original_create_engine = create_engine\n",
    "create_engine = lambda *args, **kwargs: MockSAEngine()\n",
    "\n",
    "# Reemplazar `pd.DataFrame.to_sql` con un mock si estás usando el engine mock\n",
    "_original_to_sql_method = pd.DataFrame.to_sql\n",
    "def mocked_to_sql(df_self, name, con, if_exists='fail', index=True, chunksize=None, dtype=None, method=None):\n",
    "    if isinstance(con, MockSAEngine) or (hasattr(con, 'connect') and isinstance(con.connect(), MockSAConnection)):\n",
    "        con.to_sql(df_self, name, con, if_exists, index) # Llama al mock de engine\n",
    "    else:\n",
    "        _original_to_sql_method(df_self, name, con, if_exists, index, chunksize, dtype, method)\n",
    "pd.DataFrame.to_sql = mocked_to_sql\n",
    "\n",
    "\n",
    "# Mock para boto3 (AWS S3) si no tienes credenciales AWS configuradas\n",
    "try:\n",
    "    import boto3\n",
    "    # Si boto3 está instalado, se intentará una conexión real.\n",
    "    # Para forzar el mock incluso con boto3 instalado, descomenta la línea:\n",
    "    # boto3.client = MagicMock(return_value=MagicMock(upload_file=lambda *args, **kwargs: print(f\"Mock S3: Uploading file {args[0]} to s3://{args[1]}/{args[2]}\")))\n",
    "    print(\"`boto3` instalado. Se intentará conexión real, si falla se informará.\")\n",
    "except ImportError:\n",
    "    print(\"`boto3` no instalado. Usando mock para operaciones S3.\")\n",
    "    import sys\n",
    "    sys.modules['boto3'] = MagicMock()\n",
    "    sys.modules['boto3'].client = MagicMock(return_value=MagicMock(upload_file=lambda *args, **kwargs: print(f\"Mock S3: Uploading file {args[0]} to s3://{args[1]}/{args[2]}\")))\n",
    "\n",
    "print(\"\\n--- ¡Mocks configurados para ejecución sin dependencias externas si no están instaladas! ---\")\n",
    "print(\"Si deseas usar conexiones reales, asegúrate de tener las librerías instaladas y ajusta las secciones de mockeo y credenciales.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guía Completa: Data Science en ETL\n",
    "\n",
    "La etapa de ETL (Extract, Transform, Load) es fundamental en cualquier proyecto de Data Science. Garantiza que los datos estén limpios, consistentes y listos para el análisis y modelado. Este notebook te guiará a través de ejemplos detallados de prácticas de Data Science en cada fase del ETL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extracción (Extract)\n",
    "\n",
    "**Objetivo:** Recolectar datos de diversas fuentes y formatos, asegurando su integridad.\n",
    "\n",
    "### Práctica 1: Extracción de Datos de una Base de Datos Relacional\n",
    "\n",
    "**Descripción:** Conectar a una base de datos SQL para extraer tablas específicas o resultados de consultas complejas.\n",
    "\n",
    "**Ejemplo Práctico:**\n",
    "Imagina que eres un Data Scientist en una empresa de e-commerce y necesitas analizar el comportamiento de compra de los clientes. Los datos de transacciones están en una base de datos PostgreSQL.\n",
    "\n",
    "*   **Fuentes de Datos:** Base de datos PostgreSQL (`orders` table, `customers` table, `products` table).\n",
    "*   **Datos a Extraer:** `order_id`, `customer_id`, `order_date`, `total_amount` de `orders`, junto con detalles del cliente y producto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- FASE 1: EXTRACCIÓN (EXTRACT) ---\\n\")\n",
    "\n",
    "### Práctica 1: Extracción de Datos de una Base de Datos Relacional ###\n",
    "\n",
    "print(\"### Práctica 1: Extracción de Datos de una Base de Datos Relacional ###\")\n",
    "\n",
    "# Configuración de conexión (usando mock si no hay DB real)\n",
    "# Si tienes una DB real (ej. PostgreSQL), reemplaza con tus credenciales:\n",
    "db_config = {\n",
    "    'host': 'localhost',\n",
    "    'database': 'testdb',\n",
    "    'user': 'user',\n",
    "    'password': 'password'\n",
    "}\n",
    "\n",
    "# Query SQL de Extracción\n",
    "sql_query = \"\"\"\n",
    "SELECT\n",
    "    o.order_id,\n",
    "    o.customer_id,\n",
    "    o.order_date,\n",
    "    o.total_amount,\n",
    "    c.name AS customer_name,\n",
    "    c.email AS customer_email,\n",
    "    p.product_name,\n",
    "    p.category,\n",
    "    p.price\n",
    "FROM orders o\n",
    "JOIN customers c ON o.customer_id = c.customer_id\n",
    "JOIN order_items oi ON o.order_id = oi.order_id\n",
    "JOIN products p ON oi.product_id = p.product_id\n",
    "WHERE o.order_date >= '2023-01-01';\n",
    "\"\"\"\n",
    "\n",
    "df_raw_orders = pd.DataFrame() # Inicializar\n",
    "\n",
    "try:\n",
    "    # Intentar conexión real con psycopg2 si está disponible, si no, usa el mock\n",
    "    import psycopg2 # Importar aquí para manejar el ImportError del mock\n",
    "    conn = psycopg2.connect(**db_config)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(sql_query)\n",
    "    # Obtener nombres de columnas de la descripción del cursor\n",
    "    column_names = [desc[0] for desc in cur.description]\n",
    "    raw_data = cur.fetchall()\n",
    "    df_raw_orders = pd.DataFrame(raw_data, columns=column_names)\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    print(\"Extracción de base de datos exitosa (real o simulada).\")\n",
    "except Exception as e:\n",
    "    print(f\"Error conectando a la DB o ejecutando query: {e}. Usando datos simulados.\")\n",
    "    # Simular datos si la conexión falla o el mock está activo\n",
    "    df_raw_orders = pd.DataFrame({\n",
    "        'order_id': [1, 2, 3, 4],\n",
    "        'customer_id': [101, 102, 101, 103],\n",
    "        'order_date': [datetime(2023, 1, 10), datetime(2023, 1, 15), datetime(2023, 2, 1), datetime(2023, 2, 5)],\n",
    "        'total_amount': [50.00, 120.50, 75.00, 30.00],\n",
    "        'customer_name': ['John Doe', 'Jane Smith', 'John Doe', 'Alice Wonderland'],\n",
    "        'customer_email': ['john.doe@example.com', 'jane.smith@example.com', 'john.doe@example.com', 'alice@example.com'],\n",
    "        'product_name': ['Laptop', 'Mouse', 'Keyboard', 'Webcam'],\n",
    "        'category': ['Electronics', 'Electronics', 'Peripherals', 'Accessories'],\n",
    "        'price': [1000.00, 25.00, 70.00, 50.00]\n",
    "    })\n",
    "\n",
    "print(\"\\nPrimeras filas de datos extraídos de la base de datos:\")\n",
    "print(df_raw_orders.head())\n",
    "\n",
    "# Herramientas a Utilizar: Python (psycopg2, sqlalchemy), Apache Airflow, DBeaver.\n",
    "# Objetivo: Obtener un conjunto de datos plano y combinado con información de pedidos, clientes y productos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Práctica 2: Extracción de Datos de una API Web\n",
    "\n",
    "**Descripción:** Conectar a una API (Application Programming Interface) para obtener datos en formato JSON o XML.\n",
    "\n",
    "**Ejemplo Práctico:**\n",
    "Eres un Data Scientist en una empresa de marketing y necesitas obtener datos de tendencias de búsqueda de Google Trends para un conjunto de palabras clave específicas.\n",
    "\n",
    "*   **Fuentes de Datos:** Google Trends API (o una API similar).\n",
    "*   **Datos a Extraer:** Volúmenes de búsqueda por palabra clave, por región y por período de tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n### Práctica 2: Extracción de Datos de una API Web ###\")\n",
    "\n",
    "keywords = ['data science', 'machine learning', 'artificial intelligence']\n",
    "start_date_api = '2023-01-01'\n",
    "end_date_api = '2023-01-07'\n",
    "api_key = \"YOUR_API_KEY\" # Reemplazar si la API real requiere autenticación\n",
    "\n",
    "all_trends_data = []\n",
    "\n",
    "print(f\"\\nExtrayendo datos de API para palabras clave: {', '.join(keywords)}\")\n",
    "\n",
    "# Simulación de respuestas de API\n",
    "mock_api_responses = {\n",
    "    'data science': {'trend_score': [80, 85, 82, 88, 90, 87, 91], 'dates': [f'2023-01-0{i+1}' for i in range(7)]},\n",
    "    'machine learning': {'trend_score': [70, 72, 75, 71, 78, 76, 79], 'dates': [f'2023-01-0{i+1}' for i in range(7)]},\n",
    "    'artificial intelligence': {'trend_score': [95, 93, 98, 96, 99, 94, 97], 'dates': [f'2023-01-0{i+1}' for i in range(7)]}\n",
    "}\n",
    "\n",
    "for keyword in keywords:\n",
    "    url = f\"https://api.example.com/trends?keyword={keyword}&start_date={start_date_api}&end_date={end_date_api}&api_key={api_key}\"\n",
    "    try:\n",
    "        # En un caso real, usarías requests.get(url)\n",
    "        # response = requests.get(url)\n",
    "        # if response.status_code == 200:\n",
    "        #     trend_data = response.json()\n",
    "        if keyword in mock_api_responses:\n",
    "            trend_data = mock_api_responses[keyword]\n",
    "            all_trends_data.append({'keyword': keyword, 'data': trend_data})\n",
    "            # print(f\"  - Datos para '{keyword}' obtenidos exitosamente (simulado).\") # Descomentar para ver logs\n",
    "        else:\n",
    "            print(f\"  - Error simulado: No se encontraron datos para '{keyword}'.\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"  - Error de conexión para {keyword}: {e}\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"  - Error decodificando JSON para {keyword}.\")\n",
    "\n",
    "# Convertir los datos a un DataFrame para una mejor visualización\n",
    "df_trends_list = []\n",
    "for entry in all_trends_data:\n",
    "    keyword = entry['keyword']\n",
    "    data = entry['data']\n",
    "    for i in range(len(data['dates'])):\n",
    "        df_trends_list.append({\n",
    "            'keyword': keyword,\n",
    "            'date': pd.to_datetime(data['dates'][i]),\n",
    "            'trend_score': data['trend_score'][i]\n",
    "        })\n",
    "df_trends = pd.DataFrame(df_trends_list)\n",
    "\n",
    "print(\"\\nPrimeras filas de datos de tendencias (API simulada):\")\n",
    "print(df_trends.head())\n",
    "\n",
    "# Herramientas a Utilizar: Python (requests, json), pytrends, Apache Airflow.\n",
    "# Objetivo: Obtener datos de tendencias de búsqueda para identificar patrones de interés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Transformación (Transform)\n",
    "\n",
    "**Objetivo:** Limpiar, enriquecer y estructurar los datos para que sean adecuados para el análisis.\n",
    "\n",
    "### Práctica 3: Limpieza y Estandarización de Datos\n",
    "\n",
    "**Descripción:** Identificar y corregir errores, inconsistencias y valores faltantes, y estandarizar formatos.\n",
    "\n",
    "**Ejemplo Práctico:**\n",
    "Después de extraer los datos de clientes de la base de datos (Ejemplo 1), te das cuenta de que los nombres de los clientes tienen errores tipográficos, los correos electrónicos pueden estar en mayúsculas/minúsculas y las fechas de registro tienen formatos variados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- FASE 2: TRANSFORMACIÓN (TRANSFORM) ---\\n\")\n",
    "\n",
    "### Práctica 3: Limpieza y Estandarización de Datos ###\n",
    "\n",
    "print(\"### Práctica 3: Limpieza y Estandarización de Datos ###\")\n",
    "\n",
    "# Datos de entrada simulados (con inconsistencias)\n",
    "data_customers_raw = {\n",
    "    'customer_id': [1, 2, 3, 4, 5, 6],\n",
    "    'customer_name': [\"John Doe\", \"  jane smith  \", \"Alice B.\", \"BOB BROWN\", \"Charlie D.\", \"Eve\"],\n",
    "    'customer_email': [\"john.doe@example.com\", \"JANE.SMITH@EXAMPLE.COM\", \"aliceb@example.com\", \"bob.brown@example.com\", None, \"eve@example.com \"],\n",
    "    'registration_date': [\"2023-01-15 10:30:00\", \"Jan 16, 2023\", \"17/01/2023\", \"2023-01-18\", None, \"2023-01-19\"]\n",
    "}\n",
    "df_customers_raw = pd.DataFrame(data_customers_raw)\n",
    "\n",
    "print(\"DataFrame de clientes crudo (con inconsistencias):\")\n",
    "print(df_customers_raw)\n",
    "\n",
    "# Realizar transformaciones\n",
    "df_cleaned_customers = df_customers_raw.copy()\n",
    "\n",
    "# 1. Eliminar espacios en blanco extra de `customer_name` y `customer_email`\n",
    "df_cleaned_customers['customer_name'] = df_cleaned_customers['customer_name'].str.strip()\n",
    "df_cleaned_customers['customer_email'] = df_cleaned_customers['customer_email'].str.strip()\n",
    "\n",
    "# 2. Convertir correos electrónicos a minúsculas\n",
    "df_cleaned_customers['customer_email'] = df_cleaned_customers['customer_email'].str.lower()\n",
    "\n",
    "# 3. Unificar formato de fechas y manejar nulos\n",
    "df_cleaned_customers['registration_date'] = pd.to_datetime(df_cleaned_customers['registration_date'], errors='coerce')\n",
    "df_cleaned_customers['registration_date'].fillna(datetime.now(), inplace=True) # Imputar con fecha actual\n",
    "df_cleaned_customers['registration_date'] = df_cleaned_customers['registration_date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# 4. Imputar valores faltantes en `customer_email` (ej. si después de strip queda None)\n",
    "df_cleaned_customers['customer_email'].fillna('unknown@example.com', inplace=True)\n",
    "\n",
    "# 5. (Opcional) Unificar capitalización de nombres (ej. Título)\n",
    "df_cleaned_customers['customer_name'] = df_cleaned_customers['customer_name'].str.title()\n",
    "\n",
    "print(\"\\nDataFrame de clientes limpio y estandarizado:\")\n",
    "print(df_cleaned_customers)\n",
    "\n",
    "# Herramientas a Utilizar: Python (pandas), Talend, OpenRefine.\n",
    "# Objetivo: Asegurar que los datos de clientes sean consistentes y de alta calidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Práctica 4: Enriquecimiento de Datos y Creación de Features\n",
    "\n",
    "**Descripción:** Combinar datos de diferentes fuentes y generar nuevas características (features) que puedan ser útiles para el modelado.\n",
    "\n",
    "**Ejemplo Práctico:**\n",
    "Tienes los datos de transacciones (Ejemplo 1) y quieres calcular métricas de clientes como el \"valor de vida del cliente\" (CLV) o la \"frecuencia de compra\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n### Práctica 4: Enriquecimiento de Datos y Creación de Features ###\")\n",
    "\n",
    "# Datos de ejemplo de pedidos (usando df_raw_orders del Ejemplo 1 para más realismo)\n",
    "df_orders_for_features = df_raw_orders[['customer_id', 'order_date', 'total_amount']].copy()\n",
    "df_orders_for_features['order_date'] = pd.to_datetime(df_orders_for_features['order_date'])\n",
    "\n",
    "# Añadir algunos datos adicionales para mostrar más features\n",
    "new_orders_data = pd.DataFrame({\n",
    "    'customer_id': [101, 102, 103],\n",
    "    'order_date': [datetime(2023, 3, 1), datetime(2023, 3, 10), datetime(2023, 3, 15)],\n",
    "    'total_amount': [90.00, 150.00, 60.00]\n",
    "})\n",
    "df_orders_for_features = pd.concat([df_orders_for_features, new_orders_data], ignore_index=True)\n",
    "df_orders_for_features['order_date'] = pd.to_datetime(df_orders_for_features['order_date']) # Asegurar el tipo datetime\n",
    "\n",
    "print(\"DataFrame de órdenes para feature engineering:\")\n",
    "print(df_orders_for_features)\n",
    "\n",
    "# Fecha actual para calcular antigüedad y recency. Fijamos una fecha para reproducibilidad.\n",
    "current_analysis_date = pd.to_datetime('2023-04-01')\n",
    "\n",
    "# 1. Número total de pedidos y monto total gastado por cliente\n",
    "df_customer_features = df_orders_for_features.groupby('customer_id')['total_amount'].agg(\n",
    "    num_orders=('total_amount', 'count'),\n",
    "    total_spent=('total_amount', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# 2. Promedio de gasto por pedido\n",
    "df_customer_features['avg_spent_per_order'] = df_customer_features['total_spent'] / df_customer_features['num_orders']\n",
    "\n",
    "# 3. Antigüedad del cliente (días desde el primer pedido)\n",
    "first_order_date = df_orders_for_features.groupby('customer_id')['order_date'].min().reset_index()\n",
    "first_order_date.rename(columns={'order_date': 'first_order_date'}, inplace=True)\n",
    "df_customer_features = pd.merge(df_customer_features, first_order_date, on='customer_id')\n",
    "df_customer_features['customer_tenure_days'] = (current_analysis_date - df_customer_features['first_order_date']).dt.days\n",
    "\n",
    "# 4. Calcular Recency (días desde el último pedido)\n",
    "last_order_date = df_orders_for_features.groupby('customer_id')['order_date'].max().reset_index()\n",
    "last_order_date.rename(columns={'order_date': 'last_order_date'}, inplace=True)\n",
    "df_customer_features = pd.merge(df_customer_features, last_order_date, on='customer_id')\n",
    "df_customer_features['recency_days'] = (current_analysis_date - df_customer_features['last_order_date']).dt.days\n",
    "\n",
    "# 5. Segmentación RFM básica (Recency, Frequency, Monetary)\n",
    "# Para el ejemplo, usaremos cuartiles.\n",
    "# Asegurarse de que haya suficientes valores únicos para qcut.\n",
    "# Si hay pocos valores, qcut puede fallar. `duplicates='drop'` ayuda a manejar esto.\n",
    "for col in ['recency_days', 'num_orders', 'total_spent']:\n",
    "    if df_customer_features[col].nunique() < 4:\n",
    "        print(f\"Advertencia: No hay suficientes valores únicos en '{col}' para un qcut de 4 cuartiles. Asignando valores de manera simplificada.\")\n",
    "        df_customer_features[f'{col}_score'] = pd.qcut(df_customer_features[col], q=df_customer_features[col].nunique(), labels=False, duplicates='drop') + 1\n",
    "    else:\n",
    "        labels_q = [1, 2, 3, 4]\n",
    "        if col == 'recency_days': # Menor recency es mejor, así que invertimos las etiquetas\n",
    "            labels_q = [4, 3, 2, 1]\n",
    "        df_customer_features[f'{col}_score'] = pd.qcut(df_customer_features[col], 4, labels=labels_q, duplicates='drop')\n",
    "\n",
    "df_customer_features['rfm_score'] = df_customer_features['recency_days_score'].astype(str) + \\\n",
    "                                   df_customer_features['num_orders_score'].astype(str) + \\\n",
    "                                   df_customer_features['total_spent_score'].astype(str)\n",
    "\n",
    "\n",
    "print(\"\\nDataFrame con features de cliente enriquecidas:\")\n",
    "print(df_customer_features)\n",
    "\n",
    "# Herramientas a Utilizar: Python (pandas, numpy, scikit-learn), Snowflake, Apache Spark.\n",
    "# Objetivo: Crear features enriquecidas para modelos de segmentación, CLV, churn, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Práctica 5: Agregación y Resumen de Datos\n",
    "\n",
    "**Descripción:** Reducir la granularidad de los datos, resumiéndolos a un nivel superior para facilitar el análisis o alimentar dashboards.\n",
    "\n",
    "**Ejemplo Práctico:**\n",
    "Necesitas construir un dashboard que muestre las ventas diarias y mensuales por categoría de producto. Los datos de transacciones están a nivel de ítem de pedido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n### Práctica 5: Agregación y Resumen de Datos ###\")\n",
    "\n",
    "# Datos de entrada simulados (ítems de pedido y productos)\n",
    "data_items = {\n",
    "    'order_date': [datetime(2023, 1, 1), datetime(2023, 1, 1), datetime(2023, 1, 2), datetime(2023, 1, 2), datetime(2023, 2, 1), datetime(2023, 2, 1)],\n",
    "    'product_id': [10, 20, 10, 30, 20, 10],\n",
    "    'quantity': [2, 1, 1, 3, 2, 1],\n",
    "    'price': [10.00, 25.00, 10.00, 5.00, 25.00, 10.00]\n",
    "}\n",
    "df_order_items = pd.DataFrame(data_items)\n",
    "\n",
    "data_products_agg = {\n",
    "    'product_id': [10, 20, 30],\n",
    "    'product_name': ['Laptop', 'Mouse', 'Keyboard'],\n",
    "    'category': ['Electronics', 'Electronics', 'Peripherals']\n",
    "}\n",
    "df_products_agg = pd.DataFrame(data_products_agg)\n",
    "\n",
    "print(\"DataFrame de ítems de pedido:\")\n",
    "print(df_order_items)\n",
    "print(\"\\nDataFrame de productos:\")\n",
    "print(df_products_agg)\n",
    "\n",
    "# 1. Unir `df_order_items` con `df_products_agg` para obtener la categoría\n",
    "df_merged_sales_agg = pd.merge(df_order_items, df_products_agg, on='product_id')\n",
    "\n",
    "# 2. Calcular el ingreso por ítem\n",
    "df_merged_sales_agg['revenue'] = df_merged_sales_agg['quantity'] * df_merged_sales_agg['price']\n",
    "\n",
    "print(\"\\nDataFrame de ventas con categorías y revenue:\")\n",
    "print(df_merged_sales_agg)\n",
    "\n",
    "# 3. Agregación diaria por categoría\n",
    "df_daily_revenue = df_merged_sales_agg.groupby([df_merged_sales_agg['order_date'].dt.date, 'category'])['revenue'].sum().reset_index()\n",
    "df_daily_revenue.rename(columns={'order_date': 'date'}, inplace=True)\n",
    "print(\"\\nIngresos Diarios por Categoría:\")\n",
    "print(df_daily_revenue)\n",
    "\n",
    "# 4. Agregación mensual por categoría\n",
    "df_merged_sales_agg['month'] = df_merged_sales_agg['order_date'].dt.to_period('M')\n",
    "df_monthly_revenue = df_merged_sales_agg.groupby(['month', 'category'])['revenue'].sum().reset_index()\n",
    "print(\"\\nIngresos Mensuales por Categoría:\")\n",
    "print(df_monthly_revenue)\n",
    "\n",
    "# Herramientas a Utilizar: Python (pandas), Snowflake, Tableau.\n",
    "# Objetivo: Proporcionar datos resumidos para dashboards de BI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Carga (Load)\n",
    "\n",
    "**Objetivo:** Almacenar los datos transformados en un destino adecuado para su uso final, ya sea un almacén de datos, un lago de datos o un sistema de archivo.\n",
    "\n",
    "### Práctica 6: Carga Incremental a un Data Warehouse\n",
    "\n",
    "**Descripción:** Insertar solo los nuevos o modificados registros en el destino, en lugar de recargar todos los datos cada vez. Esto es eficiente para grandes volúmenes de datos que cambian constantemente.\n",
    "\n",
    "**Ejemplo Práctico:**\n",
    "Después de transformar los datos de transacciones diarias (Ejemplo 5), necesitas cargarlos en una tabla de hechos (`fact_sales`) en tu Data Warehouse (ej. Snowflake). Solo quieres añadir los registros del día actual, no duplicar los datos históricos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- FASE 3: CARGA (LOAD) ---\\n\")\n",
    "\n",
    "### Práctica 6: Carga Incremental a un Data Warehouse ###\n",
    "\n",
    "print(\"### Práctica 6: Carga Incremental a un Data Warehouse ###\")\n",
    "\n",
    "# Datos de ventas diarias transformados (usando df_daily_revenue del Ejemplo 5)\n",
    "df_daily_sales_to_load = df_daily_revenue.copy()\n",
    "df_daily_sales_to_load['sale_date'] = df_daily_sales_to_load['date'] # Renombrar para ser consistente\n",
    "del df_daily_sales_to_load['date'] # Eliminar columna original\n",
    "\n",
    "# Añadir una columna de métrica adicional para más realismo\n",
    "df_daily_sales_to_load['num_transactions'] = (df_daily_sales_to_load['total_revenue'] / 10).astype(int) # Simulado\n",
    "df_daily_sales_to_load.rename(columns={'total_revenue': 'total_revenue'}, inplace=True) # Mantener nombre\n",
    "\n",
    "print(\"\\nDatos diarios a cargar en el Data Warehouse:\")\n",
    "print(df_daily_sales_to_load)\n",
    "\n",
    "# Configuración de conexión a Snowflake (o cualquier otra DB SQL)\n",
    "# Reemplazar con credenciales reales si se desea una conexión real.\n",
    "user = \"YOUR_SNOWFLAKE_USER\"\n",
    "password = \"YOUR_SNOWFLAKE_PASSWORD\"\n",
    "account = \"YOUR_SNOWFLAKE_ACCOUNT\"\n",
    "warehouse = \"YOUR_SNOWFLAKE_WAREHOUSE\"\n",
    "database = \"YOUR_SNOWFLAKE_DATABASE\"\n",
    "schema = \"YOUR_SNOWFLAKE_SCHEMA\"\n",
    "\n",
    "# Crear la URL de conexión para SQLAlchemy. Usará el mock si no está configurado.\n",
    "snowflake_url = f\"snowflake://{user}:{password}@{account}/{database}/{schema}?warehouse={warehouse}\"\n",
    "engine = create_engine(snowflake_url) # Esto usará nuestro mock si no hay una conexión real\n",
    "\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        # Fecha de los datos que se van a cargar (tomamos la primera fecha del DataFrame)\n",
    "        load_date = df_daily_sales_to_load['sale_date'].iloc[0].strftime('%Y-%m-%d')\n",
    "        print(f\"\\nProcesando carga incremental para la fecha: {load_date}\")\n",
    "\n",
    "        # 1. Eliminar datos existentes para la fecha actual (para idempotencia)\n",
    "        # Esto es un patrón \"delete-then-insert\" o \"upsert\" si la DB lo soporta.\n",
    "        delete_sql = text(f\"DELETE FROM fact_sales WHERE sale_date = '{load_date}';\")\n",
    "        connection.execute(delete_sql)\n",
    "        connection.commit() # Confirmar la eliminación (en el mock esto no hace nada real)\n",
    "\n",
    "        print(f\"Eliminados datos existentes (si los había) para {load_date} en 'fact_sales' (simulado).\")\n",
    "\n",
    "        # 2. Insertar los nuevos registros\n",
    "        df_daily_sales_to_load.to_sql('fact_sales', con=connection, if_exists='append', index=False)\n",
    "        connection.commit() # Confirmar la inserción (en el mock esto no hace nada real)\n",
    "\n",
    "        print(f\"Cargados nuevos datos para {load_date} en 'fact_sales' (simulado).\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error durante la carga a Data Warehouse: {e}\")\n",
    "    print(\"Asegúrate de las credenciales y configuración del DW si intentas una conexión real.\")\n",
    "finally:\n",
    "    if 'engine' in locals() and engine:\n",
    "        engine.dispose()\n",
    "    print(\"Conexión al Data Warehouse cerrada (simulada o real).\")\n",
    "\n",
    "# Herramientas a Utilizar: Python (pandas, sqlalchemy), Snowflake, Apache Airflow.\n",
    "# Objetivo: Mantener el Data Warehouse actualizado de manera eficiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Práctica 7: Carga a un Lago de Datos (Data Lake)\n",
    "\n",
    "**Descripción:** Almacenar datos brutos o semi-estructurados en su formato original o casi original, generalmente en un sistema de archivos distribuido (como HDFS) o almacenamiento de objetos (como S3).\n",
    "\n",
    "**Ejemplo Práctico:**\n",
    "Quieres almacenar los datos brutos de la API de Google Trends (Ejemplo 2) en un bucket S3 para futuros análisis o para ser procesados por otras herramientas de Big Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n### Práctica 7: Carga a un Lago de Datos (Data Lake) ###\")\n",
    "\n",
    "# Datos de ejemplo de Google Trends (usando all_trends_data del Ejemplo 2)\n",
    "print(\"\\nDatos de tendencias a cargar en el Data Lake (Formato JSON):\")\n",
    "print(json.dumps(all_trends_data, indent=2))\n",
    "\n",
    "# Nombre del bucket y prefijo de la carpeta (ej. basado en la fecha de carga)\n",
    "bucket_name = 'your-data-lake-bucket-name' # ¡IMPORTANTE! Reemplaza con un bucket S3 real si lo usas.\n",
    "current_date_str = datetime.now().strftime('%Y-%m-%d')\n",
    "file_name = f\"google_trends_data_{current_date_str}.json\"\n",
    "local_file_path = f\"/tmp/{file_name}\" # Ruta temporal para el archivo\n",
    "s3_key = f\"raw_data/google_trends/{current_date_str}/{file_name}\" # Path en S3\n",
    "\n",
    "# Crear el directorio /tmp si no existe (importante en algunos entornos como notebooks)\n",
    "os.makedirs('/tmp', exist_ok=True)\n",
    "\n",
    "# Guardar los datos a un archivo local temporalmente antes de subir\n",
    "try:\n",
    "    with open(local_file_path, 'w') as f:\n",
    "        json.dump(all_trends_data, f, indent=4)\n",
    "    print(f\"\\nArchivo temporal creado en: {local_file_path}\")\n",
    "\n",
    "    # Inicializar cliente S3 (usará el mock si boto3 no está configurado/instalado)\n",
    "    s3 = boto3.client('s3',\n",
    "                      aws_access_key_id=\"YOUR_AWS_ACCESS_KEY\",       # Reemplazar con credenciales reales\n",
    "                      aws_secret_access_key=\"YOUR_AWS_SECRET_KEY\",   # Reemplazar con credenciales reales\n",
    "                      region_name=\"YOUR_AWS_REGION\")                 # Reemplazar con la región real\n",
    "\n",
    "    # Subir el archivo a S3\n",
    "    s3.upload_file(local_file_path, bucket_name, s3_key)\n",
    "    print(f\"Archivo '{file_name}' cargado exitosamente en s3://{bucket_name}/{s3_key} (simulado o real).\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error cargando archivo a S3: {e}\")\n",
    "    print(\"Asegúrate de que las credenciales de AWS estén configuradas (env variables, ~/.aws/credentials) o reemplaza con tus claves y el bucket exista.\")\n",
    "finally:\n",
    "    # Limpiar el archivo temporal\n",
    "    if os.path.exists(local_file_path):\n",
    "        os.remove(local_file_path)\n",
    "        print(f\"Archivo temporal '{local_file_path}' eliminado.\")\n",
    "\n",
    "# Herramientas a Utilizar: Python (boto3), Amazon S3, Apache Airflow.\n",
    "# Objetivo: Almacenar datos brutos en un formato flexible y escalable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consideraciones Clave para Data Science en ETL:\n",
    "\n",
    "\n",
    "*   **Calidad de Datos:** Un ETL robusto es la base para modelos de ML precisos. Datos basura (garbage in) resultan en modelos basura (garbage out).\n",
    "*   **Observabilidad:** Monitorear los pipelines ETL para detectar fallos, latencias o problemas de calidad de datos es fundamental.\n",
    "*   **Versionado:** Versionar tanto el código ETL como los esquemas de datos y los datos mismos ayuda a la reproducibilidad.\n",
    "*   **Idempotencia:** Asegurarse de que una re-ejecución del ETL no cause efectos secundarios no deseados (ej. duplicación de datos).\n",
    "*   **Escalabilidad:** Diseñar el ETL para manejar volúmenes crecientes de datos.\n",
    "*   **Automatización:** Utilizar orquestadores para automatizar la ejecución programada de los pipelines.\n",
    "*   **Documentación:** Documentar cada paso del ETL, las fuentes, transformaciones y destinos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- FIN DEL SCRIPT ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
